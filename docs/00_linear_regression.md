# Linear Regression - Your First ML Model

## ðŸŽ“ What is Linear Regression?

Linear regression is the simplest machine learning algorithm. It finds the best straight line (or hyperplane) that fits your data.

### The Model

```
y = Xw + b
```

Where:
- **y** = predictions (what we're trying to predict)
- **X** = input features (data we have)
- **w** = weights (slope of the line)
- **b** = bias (y-intercept)

### The Goal

Find the values of **w** and **b** that minimize the prediction error!

---

## ðŸ“ Files

### [linreg.ex](file:///home/rajesh/lab/elixir/ml/ml_nx/lib/ml_nx/linreg.ex)

Core implementation with:
- `predict/3` - Make predictions: Å· = Xw + b
- `mse/2` - Mean Squared Error loss
- `loss/4` - Compute loss for given parameters
- `step/5` - One gradient descent step
- `train/3` - Full training loop

### [linreg_test.exs](file:///home/rajesh/lab/elixir/ml/ml_nx/test/linreg_test.exs)

Tests covering:
- Prediction with single and multiple features
- MSE calculation
- Training convergence
- Parameter learning

### [00_linreg_demo.exs](file:///home/rajesh/lab/elixir/ml/ml_nx/examples/00_linreg_demo.exs)

Interactive examples:
1. Simple linear relationship (y = 3x + 2)
2. Multiple features (y = 2xâ‚ + 3xâ‚‚ + 1)
3. House price prediction

---

## ðŸ”‘ Key Concepts

### 1. The Prediction Function

```elixir
defn predict(x, w, b) do
  Nx.dot(x, w) + b
end
```

This is just matrix multiplication plus a bias term!

### 2. Mean Squared Error

```elixir
defn mse(y_hat, y) do
  err = y_hat - y
  Nx.mean(err * err)
end
```

Measures how far off our predictions are (on average).

### 3. Gradient Descent Step

```elixir
defn step(x, y, w, b, lr) do
  {gw, gb} = grad({w, b}, fn {w, b} ->
    loss(x, y, w, b)
  end)
  
  w = w - lr * gw
  b = b - lr * gb
  
  {w, b}
end
```

Updates weights and bias to reduce the loss.

### 4. Training Loop

```elixir
def train(x, y, opts \\ []) do
  iters = Keyword.get(opts, :iters, 500)
  lr = Keyword.get(opts, :lr, 0.05)
  
  # Initialize parameters
  {_, d} = Nx.shape(x)
  w = Nx.broadcast(0.0, {d})
  b = Nx.tensor(0.0)
  
  # Iterate to minimize loss
  Enum.reduce(1..iters, {w, b}, fn _, {w, b} ->
    step(x, y, w, b, lr)
  end)
end
```

Repeatedly applies gradient descent until convergence.

---

## ðŸ“Š Example: Learning y = 3x + 2

```elixir
# Training data
x = Nx.tensor([[1.0], [2.0], [3.0], [4.0], [5.0]])
y = Nx.tensor([5.0, 8.0, 11.0, 14.0, 17.0])

# Train the model
{w, b} = MLNx.LinReg.train(x, y, iters: 1000, lr: 0.01)

# Results
# w â‰ˆ 3.0 (slope)
# b â‰ˆ 2.0 (intercept)
```

The model successfully learned the relationship!

---

## ðŸŽ¯ What You Learned

âœ… Linear regression finds the best line through data  
âœ… Model: y = Xw + b  
âœ… Loss: Mean Squared Error  
âœ… Optimization: Gradient Descent  
âœ… Training: Iterate until loss is minimized  

---

## ðŸ”— Connection to Later Lessons

This simple linear regression contains ALL the core ML concepts:

1. **Model** (predict function) â†’ Neural networks are just more complex models
2. **Loss Function** (MSE) â†’ Commit 2 explores more loss functions
3. **Gradient Descent** (step function) â†’ Commit 1 deep dives into this
4. **Training Loop** (train function) â†’ Foundation for all ML training

Linear regression is the perfect starting point because it's simple but contains everything you need to understand more complex models!

---

## ðŸš€ Run the Code

```bash
# Run tests
mix test test/linreg_test.exs

# Run demo
mix run examples/00_linreg_demo.exs
```

---

## ðŸ“š Mathematical Details

### Why MSE?

MSE = (1/n) Î£(Å·áµ¢ - yáµ¢)Â²

- Differentiable (smooth gradients)
- Penalizes large errors heavily
- Convex (one global minimum)

### Gradient Computation

Using automatic differentiation (`grad`):

```
âˆ‚MSE/âˆ‚w = (2/n) Î£ xáµ¢(Å·áµ¢ - yáµ¢)
âˆ‚MSE/âˆ‚b = (2/n) Î£(Å·áµ¢ - yáµ¢)
```

Nx computes these automatically!

### Update Rule

```
w_new = w_old - Î± * âˆ‚MSE/âˆ‚w
b_new = b_old - Î± * âˆ‚MSE/âˆ‚b
```

Where Î± (alpha) is the learning rate.

---

## ðŸŽ“ Next Steps

Now that you understand linear regression, you're ready to learn:

- **Commit 1**: Deep dive into gradient descent
- **Commit 2**: Different loss functions (MAE, Huber, Cross-Entropy)
- **Commit 3**: Regularization (preventing overfitting)

Linear regression is your foundation - everything builds on these concepts! ðŸš€
